---
title: "FA3 Data Mining and Wrangling"
author: "Bayquen, Christopher Gilbert A."
date: "2024-03-11"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, global_options, include=FALSE}
knitr::opts_chunk$set(message=FALSE, tidy.opts=list(width.cutoff=60), tidy=TRUE) 
```

## **CASE STUDY: Bone Mineral Density**

```{r echo=FALSE}
#exporting the data set and necessary libraries
library(tidyverse) # tidyverse
library(readxl) # for reading Excel files


bmd_df <- read_excel("bmd-data.xlsx")
```


## 1.1 Import
```{r}
bmd_raw <- as_tibble(bmd_df)
bmd_raw
```
## 1.2 Tidy 

The layout of the data in the tibble seems to be in a wide format, where each row corresponds to an individual child (identified by idnum), and there are columns for various attributes such as age, sex, fracture, weight_kg, height_cm, medication, waiting_time, and spnbmd (spinal bone mineral density). To get the data into tidy format, we typically want each variable to have its own column and each observation to have its own row. In tidy format, the variables would include idnum, age, sex, fracture, weight_kg, height_cm, medication, waiting_time, and spnbmd. Since we have categorical values under sex, fracture, and medication, we need to convert them into factors using *as.factor()* for better representation.

*tidying up the data*

```{r}

library(dplyr)

bmd_tidy <- bmd_raw %>%
  mutate(
    sex = as.factor(sex),
    fracture_status = as.factor(fracture),
    medication = as.factor(medication)
    )
   
bmd_tidy

```

## 1.3 Explore 

```{r}
#number of children
children <- nrow(bmd_tidy[bmd_tidy$age < 18, ])

# Counting the number of boys and girls
boys <- nrow(bmd_tidy[bmd_tidy$sex == "M", ])
girls <- nrow(bmd_tidy[bmd_tidy$sex == "F", ])

# Median ages of boys and girls
median_age_boys <- median(bmd_tidy$age[bmd_tidy$sex == "M"], na.rm = TRUE)
median_age_girls <- median(bmd_tidy$age[bmd_tidy$sex == "F"], na.rm = TRUE)

```

Among the 169 People in the table, there are **0 children (<18 Years old)** in the data set. The youngest of the patients is around 35 years old while the oldest is 88. There are **86 Male patients** and **83 Female patients** having **63.2333 years old** as the median age for the males, and **63.7695 years old** for the females.

```{r echo=FALSE}
library(ggplot2)
library(cowplot) # for side by side plots

# Plotting spnbmd distributions
spnbmdplot <-ggplot(data = bmd_tidy, aes(x = spnbmd, fill = sex)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 20) +
  labs(title = "Distribution of spnbmd by Gender", x = "spinal bone mineral density") +
  theme_minimal()

# Plotting age distributions
ageplot <- ggplot(data = bmd_tidy, aes(x = age, fill = sex)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 20) +
  labs(title = "Distribution of Age by Gender", x = "Age") +
  theme_minimal()

plot_grid(spnbmdplot, ageplot, nrow = 1)
```
We can observe in both of the graphs that the data provided is generally distributed normally, however, we can see that between the two genders, the variation between the amount of spinal bone mineral density is greater with the male patients than of the female patients, and that the age of the age of the male patients also varies more than the female patients. There are more female patients at the age of 65-70 years old, while the male patients seem to have greater spinal bone mineral density.

```{r}
# Scatter plot of spnbmd versus age, faceting by gender
scatter_plot <- ggplot(data = bmd_tidy, aes(x = age, y = spnbmd, color = sex)) +
  geom_point() +
  labs(title = "Scatter Plot of spnbmd vs Age by Gender", x = "Age", y = "spinal bone mineral density") +
  facet_wrap(~sex) +
  theme_minimal()

scatter_plot
```
In the plot above, we can see that between the two genders, a similar trend can be seen throughout the data; as the patients grow older, the spinal bone mineral density seems to go lower (given that there are a few outliers for each age group per gender).


## 1.4 MODEL

## 1.4.1 Split
```{r echo = FALSE}
# Set seed for reproducibility
set.seed(5)

# Define the train_samples indices
train_samples <- sample(1:nrow(bmd_tidy), round(0.8 * nrow(bmd_tidy)))

# Split the data into training and test sets
bmd_train <- bmd_tidy[train_samples, ]
bmd_test <- bmd_tidy[-train_samples, ]
```

*bmd train subset*
```{r echo=FALSE}
bmd_train 
```
*bmd test subset*
```{r echo=FALSE}
bmd_test 
```

## 1.4.2 Tune

1. Separate bmd_train into bmd_train_male and bmd_train_female, and likewise
for bmd_test.

```{r}
# Separate bmd_train into bmd_train_male and bmd_train_female
bmd_train_male <- bmd_train %>%
  filter(sex == "M")

bmd_train_female <- bmd_train %>%
  filter(sex == "F")

# Separate bmd_test into bmd_test_male and bmd_test_female
bmd_test_male <- bmd_test %>%
  filter(sex == "M")

bmd_test_female <- bmd_test %>%
  filter(sex == "F")
```

2. Using cross_validate_spline from the stat471 R package, perform 10-fold cross-validation on
bmd_train_male and bmd_train_female, trying degrees of freedom 1,2,. . . ,15.


\newpage

## 2 KNN AND BIAS-VARIANCE TRADEOFF

The yield Y of the tree planted E_1 meters to the right and E_2 meters up from the
bottom left-hand corner of the orchard has distribution $Y = f(E) + \epsilon$, where

$$f(E) = 50 + 0.001E^2_1 + 0.001E^2_2, \epsilon=N(0,\sigma^2 ), \sigma = 4$$

## 2.1 A simple rule to predict this season’s yield
 
This apple season is right around the corner, and you’d like to predict the yield of each tree. You come up with perhaps the simplest possible prediction rule: predict this year’s yield for any given tree based on last year’s yield from that same tree. Without doing any programming, answer the following questions:

1. What is the training error of such a rule?

$$\text{Training Error} = \frac{1}{N} \sum_{i=1}^{N} (Y^{test}_i - \hat{Y}^\text{test}_{i})^2$$
2. What is the mean squared bias, mean variance, and expected test error of this prediction rule?

- The mean squared bias is the squared difference between the expected prediction and the true value averaged over different training sets.

$$\text{MSB} = E[(E[Y]-\hat{Y})^2]$$
 
- The mean variance is the variance of the predictions averaged over different training sets.

$$\text{MV} = E[Var(\hat{Y})]$$
- The expected test error is the expected value of the squared difference between the true value and the prediction.

$$ \text{Expected Test Error} = \frac{1}{N} \sum_{i=1}^{N} E(Y^{test}_i - \hat{f}(X^\text{test}_{i}))^2$$
## 2.2 K-nearest neighbors regression (conceptual)

As a second attempt to predict a yield for each tree, you average together last year’s yields of the K trees
closest to it (including itself, and breaking ties randomly if necessary). So if you choose K = 1, you get back
the simple rule from the previous section. KNN is not a parametric model like linear or logistic regression, so it is a little harder to pin down its degrees
of freedom.

1. What happens to the model complexity as K increases? Why?

- As K increases in K-nearest neighbors (KNN) regression, the model complexity decreases. This happens because with a larger K, the prediction for each data point is based on a larger number of neighboring points. Consequently, the model becomes smoother and less sensitive to local fluctuations in the data. The decision boundary becomes more linear and less flexible as K increases.

2. The degrees of freedom for KNN is sometimes considered n/K, where n is the training set size. Why
might this be the case?

- The degrees of freedom for KNN is sometimes considered as n/K, where n is the training set size. This is because in KNN, the model fits the data by storing all the training instances. With a largerK, the model uses more training instances to make predictions. Therefore, the effective number of parameters or degrees of freedom decreases as the number of neighbors considered increases. In situations where the data are clumped in groups of K, each group essentially contributes one parameter to the model. Hence, the effective degrees of freedom are reduced by a factor of K.

3. Conceptually, why might increasing K tend to improve the prediction rule? What does this have to do
with the bias-variance tradeoff?

- Increasing K tends to improve the prediction rule because it reduces the variance of the model. By considering more neighbors, the model becomes more stable and less sensitive to noise or outliers in the data. This reduction in variance helps to avoid overfitting and leads to better generalization to unseen data. It contributes to a decrease in model complexity and, consequently, a decrease in variance. This is related to the bias-variance tradeoff because reducing variance typically increases bias. However, in many cases, the reduction in variance outweighs the increase in bias, leading to overall better predictive performance.

4. Conceptually, why might increasing K tend to worsen the prediction rule? What does this have to do
with the bias-variance tradeoff?

- Increasing K may tend to worsen the prediction rule because it can increase bias while decreasing variance. With a larger K, the model becomes overly simplified and may fail to capture the true underlying relationships in the data. It may lead to underfitting, where the model is too rigid and cannot capture the complexity of the data.
This degradation in performance is also related to the bias-variance tradeoff. While reducing variance by increasing K, the model may become too biased and fail to capture the true patterns in the data, leading to poor generalization















